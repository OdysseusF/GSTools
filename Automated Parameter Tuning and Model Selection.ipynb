{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d971171a-7547-4756-8244-f9484fe63322",
   "metadata": {},
   "source": [
    "# Today, our goal is to produce an automated set of codes that can help us do parameter tuning and model selection. \n",
    "## We aim to write down reproducible codes that can be useful for future purposes, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cbc93-2a22-49a2-b3f5-964bef7c8727",
   "metadata": {},
   "source": [
    "### To get started, we load the relevant packages and performance the standard operations in data preprossing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d557472-5b35-4cba-8891-8286a15f01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gstools as gs\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/Users/cui/Library/CloudStorage/OneDrive-YaleUniversity/0 High-Dim Spatial/hubmap/ann/B009A_22_03_03_Skywalker_reg001_compensated_ann.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "# shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# pick a protein and drop the rows with missing values\n",
    "protein = 'Synapto'\n",
    "\n",
    "df = df.dropna(subset=[protein])\n",
    "\n",
    "# we normalise x and y so that the grid is approximately 1 by 1\n",
    "\n",
    "df['x'] = df['x']/df['x'].max()\n",
    "df['y'] = df['y']/df['y'].max()\n",
    "\n",
    "df_train = df.head(2000)\n",
    "df_test = df.tail(1000)\n",
    "x_train = df_train['x']\n",
    "y_train = df_train['y']\n",
    "val_train = df_train[protein]\n",
    "\n",
    "x_test = df_test['x']\n",
    "y_test = df_test['y']\n",
    "val_test = df_test[protein]\n",
    "\n",
    "# grid definition for output field\n",
    "gridx = np.arange(0.0, 1.05, 0.05)\n",
    "gridy = np.arange(0.0, 1.05, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee54d5-43d8-4e15-a6f7-d1495f170922",
   "metadata": {},
   "source": [
    "### Next, we study an example generated by ChatGPT of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4832dd-9c14-44cc-b1fc-67daf3f76c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: beta0 = 0.017400463340610635, beta1 = 1.9951803506719779\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(x, y, learning_rate=0.01, iterations=1000):\n",
    "    n = len(y)\n",
    "    beta0 = 0\n",
    "    beta1 = 0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        y_pred = beta0 + beta1 * x\n",
    "        error = y - y_pred\n",
    "        \n",
    "        # Compute the gradients\n",
    "        d_beta0 = (-2/n) * sum(error)\n",
    "        d_beta1 = (-2/n) * sum(error * x)\n",
    "        \n",
    "        # Update parameters\n",
    "        beta0 = beta0 - learning_rate * d_beta0\n",
    "        beta1 = beta1 - learning_rate * d_beta1\n",
    "    \n",
    "    return beta0, beta1\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "beta0, beta1 = gradient_descent(x, y)\n",
    "print(f\"Optimal parameters: beta0 = {beta0}, beta1 = {beta1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d99401-4456-4c06-aaf3-64888e492586",
   "metadata": {},
   "source": [
    "### While this is meaningful, we need to be able to write the derivative of loss w.r.t. the parameters.  How can this be done if we do not yet know the explicit form of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "903ca905-dc24-4511-950a-54eb01fc13c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensor\n\u001b[1;32m      7\u001b[0m training_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mFashionMNIST(\n\u001b[1;32m      8\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m     download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     transform\u001b[38;5;241m=\u001b[39mToTensor()\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
